{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport sklearn\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T09:04:13.471237Z","iopub.execute_input":"2025-05-02T09:04:13.471668Z","iopub.status.idle":"2025-05-02T09:04:15.265976Z","shell.execute_reply.started":"2025-05-02T09:04:13.471634Z","shell.execute_reply":"2025-05-02T09:04:15.264827Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# чистка выбросов\n\nimport pandas as pd\n\n# Example DataFrame\ndf = pd.DataFrame({'value': [12, 15, 14, 10, 102, 13, 11, 14, 13, 15, 110]})\n# Compute IQR boundaries\nQ1 = df['value'].quantile(0.25)\nQ3 = df['value'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Filter out outliers outside the IQR bounds\ndf_clean = df[~((df['value'] < lower_bound) | (df['value'] > upper_bound))]\nprint(df_clean)\n\n\n#resampling\nfrom imblearn.over_sampling import SMOTE\n\nX_train, y_train = ...  # your feature matrix and labels for training\nprint(\"Class distribution before:\", dict(pd.Series(y_train).value_counts()))\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X_train, y_train)\nprint(\"Class distribution after:\", dict(pd.Series(y_res).value_counts()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NLP","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)\n\nctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)\n\n\n# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef spacy_preprocess(text):\n    doc = nlp(text)\n    return [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nsentences = [[\"i\", \"like\", \"nlp\"], [\"word\", \"embeddings\", \"are\", \"cool\"]]\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\nvector = model.wv['nlp']\nprint(vector[:5])  # пример вектора\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T14:25:26.349287Z","iopub.execute_input":"2025-05-02T14:25:26.350116Z","iopub.status.idle":"2025-05-02T14:25:59.778234Z","shell.execute_reply.started":"2025-05-02T14:25:26.350084Z","shell.execute_reply":"2025-05-02T14:25:59.777505Z"}},"outputs":[{"name":"stdout","text":"[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193 ]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\n\ndef load_glove(file_path):\n    embeddings = {}\n    with open(file_path, encoding=\"utf8\") as f:\n        for line in f:\n            parts = line.split()\n            word = parts[0]\n            vector = np.array(parts[1:], dtype=\"float32\")\n            embeddings[word] = vector\n    return embeddings\n\nglove = load_glove(\"glove.6B.100d.txt\")\nprint(glove[\"king\"][:5])\n","metadata":{"trusted":true,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import FastText\n\nsentences = [[\"i\", \"like\", \"natural\", \"language\", \"processing\"]]\nmodel = FastText(sentences, vector_size=100, window=3, min_count=1)\nprint(model.wv[\"language\"][:5])\n\n#или\n\nimport fasttext\nimport fasttext.util\nfasttext.util.download_model('en', if_exists='ignore')  # скачивает модель\n\nft = fasttext.load_model('cc.en.300.bin')\nprint(ft.get_word_vector(\"hello\")[:5])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()\n\ndef nltk_lemmatize(text):\n    tokens = word_tokenize(text)\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    return tokens\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(tokens):\n    return [t for t in tokens if t not in stop_words]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nfrom keras.layers import Dense, GRU, Dropout, Activation, Embedding, SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.model import Sequential\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# размер вокаба\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#полный пайплайн\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nlemmatizer_en = WordNetLemmatizer()\n\nstop_words_en = set(stopwords.words('english'))\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text) #числа\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'[^a-z]', ' ', text)\n    \n    tokens = word_tokenize(text, language='english')\n    \n    tokens = [word for word in tokens if word not in stop_words_en and len(word) > 2]\n    pos_tags = nltk.pos_tag(tokens)\n    tokens = [lemmatizer_en.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n    \n    return tokens\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T17:00:05.515454Z","iopub.execute_input":"2025-05-02T17:00:05.515847Z","iopub.status.idle":"2025-05-02T17:02:45.597190Z","shell.execute_reply.started":"2025-05-02T17:00:05.515820Z","shell.execute_reply":"2025-05-02T17:02:45.595993Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n[nltk_data]     [Errno -3] Temporary failure in name resolution>\n[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n[nltk_data] Error loading omw-1.4: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\nclf1 = LogisticRegression()\nclf2 = RandomForestClassifier(n_estimators=100)\nclf3 = svm.SVC(probability=True)\n\nensemble = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='hard')\nensemble.fit(X_train, y_train)\ny_pred = ensemble.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}